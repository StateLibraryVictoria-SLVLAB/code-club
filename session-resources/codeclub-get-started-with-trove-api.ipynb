{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThis notebook uses the Trove API to introduce some core concepts for using Web APIs.\nTopics covered include:\n\n- important terms\n- using API keys and safely managing secrets\n- using the Requests library to make GET requests\n- getting the data you want using header and query parameters\n- working with JSON data\n- where to go next...\n\nMuch of the information in this guide is drawn from the [Trove Data Guide](https://tdg.glam-workbench.net/home.html) and the [Trove API technical guide](https://trove.nla.gov.au/about/create-something/using-api/v3/api-technical-guide) - which have in-depth information on all aspects of the Trove API.\n\n","metadata":{}},{"cell_type":"markdown","source":"## What are APIs for?\n\nApplication Programming Interfaces (APIs) are interactive bits of code developers create to allow other programs to use their tools. You could think of them like USB ports, where many different devices can connect, so long as they are built with the USB design standard. This is a bit of an oversimplification, but the idea is that APIs make it easier to share and update code without breaking things that connect to them.\n\nThe Trove API is representative of a family of APIs broadly known as web APIs, which tend to follow a similar design pattern. The Trove API works similarly to a website URL in a browser: the URL points to the location of the webpage, the data is retrieved from that location, and the browser displays it. The Trove API follows a similar process, but instead of returning a web page it returns structured data. To see what this looks like in a browser click this link: [https://api.trove.nla.gov.au/v3/newspaper/titles](https://api.trove.nla.gov.au/v3/newspaper/titles).\n\nWeb APIs become powerful when you combine them with code to use the structured data in some way. To see an example, run the code block below. Don't worry about what it is doing yet - all the processes will be explained throughout the notebook.","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\nfrom IPython.display import HTML, display\nimport random\n\n# a function to create a clickable thumbnail.\ndef render_image(source_url, thumbnail_url):\n    display(HTML('<a href=\"{}\"><img src=\"{}\"></a>'.format(source_url, thumbnail_url)))\n\n# a function to check that user input is a year\ndef is_year(string):\n    try:\n        if not 0 <= int(string) <= 2024:\n            print(\"Year must be between 0 and 2024\")\n            return False\n        return True\n    except:\n        print(\"Input must be an integer.\")\n        return False\n    \ndef get_trove_images(word, year):\n    if is_year(year):\n        # create API request\n        search_url = \"https://api.trove.nla.gov.au/v3/result\"\n        query = {\"category\":\"image\", \"q\":word, \"l-year\":f\"{year}\", \"n\":\"100\"}\n        encoding = {\"accept\":\"application/json\"}\n        response = requests.get(search_url, params=query, headers=encoding)\n        \n        # Load the data as JSON\n        data = response.json()\n        images = data[\"category\"][0].get(\"records\")\n        \n        # Let the user know how many results the query has\n        total = images[\"total\"]\n        print(f\"Your query had {total} results.\")\n        if total == 0:\n            print(\"Run the cell again to retry.\")\n            return\n        \n        # set the number of results to max 100 (the number returned by a single API call)\n        if total > 100:\n            total = 100\n        numbers = []\n        source = None\n        \n        # check for thumbnails not from SLV and display them, with their title and date information.\n        while (len(numbers) < total):\n            number = random.randrange(1, total)\n            if number not in numbers:\n                try:\n                    thumbnail = images[\"work\"][number][\"identifier\"][0][\"value\"]\n                    if thumbnail is not None and \"slv\" not in thumbnail:\n                        # print(images[\"work\"][number])\n                        source = images[\"work\"][number][\"troveUrl\"]\n                        title = images[\"work\"][number][\"title\"]\n                        date = images[\"work\"][number][\"issued\"]\n                        break\n                    else:\n                        numbers.append(number)\n                except:\n                    numbers.append(number)\n                    continue\n        if source is not None:\n            render_image(source, thumbnail)\n            print(title + \", \" + date)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:27:48.255864Z","iopub.execute_input":"2024-08-14T05:27:48.256354Z","iopub.status.idle":"2024-08-14T05:27:48.274664Z","shell.execute_reply.started":"2024-08-14T05:27:48.256317Z","shell.execute_reply":"2024-08-14T05:27:48.273231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have setup the underlying logic, we can call `get_trove_images()` to search the Trove database. The code will prompt you for a year that will be used to filter the results. It is already setup to search for the word `computer`, but you can update this term to anything that interests you. If something goes wrong it will go back to 2000, as it was a simpler time.\n\nTrove aggregates a lot of data, so some thumbnails won't display correctly. Running the code a few times will eventually find a nice thumbnail that can be rendered. The thumbnails also have a link to the Trove page for the resource, so you can click through to view more information.","metadata":{}},{"cell_type":"code","source":"print(\"Enter a year...\")\ntry:\n    year = input()\n    get_trove_images(\"computer\", year)\nexcept Exception as e:\n    print(\"Whoops, something went wrong... Reverting to y2k...\")\n    get_trove_images(\"computer\",\"2000\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:27:48.277121Z","iopub.execute_input":"2024-08-14T05:27:48.277522Z","iopub.status.idle":"2024-08-14T05:28:41.505112Z","shell.execute_reply.started":"2024-08-14T05:27:48.277490Z","shell.execute_reply":"2024-08-14T05:28:41.503937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting started\n\n#### Important terms:\n- API : application programming interface - a way for computers to communicate with each other.\n- Web API : an API designed to allow communication to happen over the web.\n- Endpoints : the location of data that can be requested via the API, these generally appear like URLs. \n- Requests : a structured command sent to the web server that prompts it to return data. \n- Header : metadata about a request or response.\n- Parameters : options that change the response. These are specific to each API and could include search terms, categories, or ids.\n- Key : a string of characters used to authenticate a connection.\n\n### Using keys and managing secrets\n\nWhen working with APIs, a key is often required to authenticate the connection. You should think of API keys as a form of password, and as such keys should **never** be stored in published code or documentation as they might be exploited. \n\nTo avoid this, keys are stored in a separate secure location and loaded into the code as variables.\n\nFor example, you may use a variable called `APIkey` in the code. Another file will have a line of code where the value is recorded. For example:\n\n        APIkey = \"The api key value\"\n        \nThen the code will load the data each time the script is run. How this looks will depend on the virtual environment being used. In a local code repository this could involve using a [.env file](https://python.plainenglish.io/the-essentials-of-env-files-in-python-simplifying-environment-management-securing-your-secrets-14c51c411400) to store the variable name and value, and a [.gitignore file](https://www.w3schools.com/git/git_ignore.asp?remote=github) which excludes the .env file from tracking in the git repository.\n\nKaggle handles secrets using an Add-on called Secrets. You can read more about it here - [Feature Launch: User Secrets](https://www.kaggle.com/discussions/product-feedback/114053).\n\nThis notebook uses a public API that does not require a key, but it is important to remember to carefully store API keys when you begin to work with them.","metadata":{}},{"cell_type":"markdown","source":"## Requests in Python\n\n### Python Requests library\n    \nThe Python [Requests library](https://requests.readthedocs.io/en/latest/) provides an easy-to-use wrapper to make API requests. It handles authentication, [percent encoding](https://www.w3schools.com/tags/ref_urlencode.ASP), and other steps that are required to make a valid API request.\n\nTo use the library, it needs to be imported into the project.\n","metadata":{}},{"cell_type":"code","source":"import requests\n# Also import the json library for printing JSON\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:41.506785Z","iopub.execute_input":"2024-08-14T05:28:41.507159Z","iopub.status.idle":"2024-08-14T05:28:41.512656Z","shell.execute_reply.started":"2024-08-14T05:28:41.507127Z","shell.execute_reply":"2024-08-14T05:28:41.511347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Base URL and endpoints\n\nEach Web API will have a base URL, which is consistent between different endpoints. The base URL for Trove is [https://api.trove.nla.gov.au/v3](https://api.trove.nla.gov.au/v3)\n\nDifferent endpoints can be appended to the base URL to access different data. \nFor example:\n\n- `/result` – functions like a search in the Trove interface.\n- `/newspaper/titles` - limits the search scope to Newspaper titles.\n- `/newspaper/title/[id]` - limits the results to a single Newspaper title by its id.\n- `/newspaper/[id]` - limits the results to a single issue of a Newspaper by its id.\n\nFor a full list of Trove endpoints see [Endpoints in the Trove Data Guide](https://wragge.github.io/trove-data-guide/accessing-data/trove-api-intro.html#endpoints).\n\n### Encoding\n\nMost web APIs will return data encoded in either JSON (JavaScript Object Notation) or XML. \nTo set the preferred encoding a header needs to be submitted as part of the request. Because it is easier to work with JSON in Python, this tutorial uses JSON so we add a header variable. Not all APIs will provide both JSON and XML, so it is important to check the documentation to figure out what is available.\n\n### Request methods\n\nThe `Requests` library supports a range of HTTP (Hypertext Transfer Protocol) request methods including:  \n- `get` - used for requesting data.\n- `put` - used for sending data to create/update something.\n\nThe Trove API is designed to give access to data, so will only support GET requests. You can read more about HTTP request methods [here](https://www.w3schools.com/tags/ref_httpmethods.asp).\n\nRun the code below to make a call for all newspaper titles in Trove using a GET request.\n","metadata":{}},{"cell_type":"code","source":"base_url = \"https://api.trove.nla.gov.au/v3\"\nendpoint = \"/newspaper/titles\"\n\n# Set the request to accept JSON\nencoding = {\"accept\":\"application/json\"}\n\n# calls the API by sending a GET request to the specified endpoint with encoding passed to headers\nresponse = requests.get(base_url + endpoint, headers=encoding)\n\n# An example bad request with a non-existent endpoint\nbad_request = requests.get(base_url + \"/something\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:41.516046Z","iopub.execute_input":"2024-08-14T05:28:41.516504Z","iopub.status.idle":"2024-08-14T05:28:44.683024Z","shell.execute_reply.started":"2024-08-14T05:28:41.516469Z","shell.execute_reply":"2024-08-14T05:28:44.681645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The response returned by the API request will be stored in each variable. The response object contains data that can be useful to view:\n\n- `response.url` will return the URL sent to Trove to make the request\n- `response.status_code` will return a number that indicates if the request was successful (read the [Status Code Documentation](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) for more information)\n- `response.text` will return the full text of the response content\n\nThe code below will show a successful call to the valid endpoint `/newspaper/titles` and a not found response for the fake endpoint `/something`.\n\nTry adding in `response.txt` for the successful response below to see the difference.","metadata":{}},{"cell_type":"code","source":"# Prints the URL and status code for the request using a valid endpoint\nprint(response.url)\nprint(f\"Status code for response: {response.status_code}\")\nif response.status_code == 200:\n    print(\"OK!\")\nelse:\n    print(\"Something went wrong :(\")\nprint(\"\")\n\n# Prints the URL and status code for the request using an invalid endpoint\nprint(bad_request.url)\nprint(f\"Status code for bad response: {bad_request.status_code}\")\nif bad_request.status_code == 200:\n    print(\"OK!\")\nelse:\n    print(\"Something went wrong :(\")\nprint(\"Data returned:\")\nprint(bad_request.text)\nprint(\"\")\n\n# Additional way to check if response code is ok\nprint(\"requests.codes.ok can also be used to test status codes:\")\nif response.status_code == requests.codes.ok:\n    print(f\"The response status code ({response.status_code}) is on the OK list.\")\nelse:\n    print(f\"The response status code ({response.status_code}) indicates something went wrong...\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.684660Z","iopub.execute_input":"2024-08-14T05:28:44.685064Z","iopub.status.idle":"2024-08-14T05:28:44.694511Z","shell.execute_reply.started":"2024-08-14T05:28:44.685030Z","shell.execute_reply":"2024-08-14T05:28:44.693370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The request can also return headers, which include useful metadata about the request such as the date and time, rate limit, and content encoding.","metadata":{}},{"cell_type":"code","source":"headers = response.headers\nprint(headers)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.696073Z","iopub.execute_input":"2024-08-14T05:28:44.696476Z","iopub.status.idle":"2024-08-14T05:28:44.713827Z","shell.execute_reply.started":"2024-08-14T05:28:44.696422Z","shell.execute_reply":"2024-08-14T05:28:44.712515Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As the request is returning the data in JSON, it needs to be loaded before it can be accessed.\n\nDepending on how the data has been structured by the developer of the API, there may be different tags that are returned. You can use the inbuilt `json.dumps()` method to *pretty print* the output, which helps us to understand the way the returned data is structured.\n\nHere is an example `json.dumps()` output for a simple JSON record, which includes the top level tag `records`, with a list of three entries each containing `record_id`, `title`, `fun_fact` and in one case `url`.","metadata":{}},{"cell_type":"code","source":"# An example string with JSON syntax\njson_string = r'{\"records\": [{\"record_id\" : \"4627\", \"title\":\"Why JSON\",\"fun_fact\":\"JSON is a standard designed to allow structured data to be exchanged on the internet.\"},{\"record_id\" : \"42\", \"title\":\"JSON and the Argonauts\",\"fun_fact\":\"JSON is pronounced like the human name Jason.\"},{\"record_id\" : \"2002\", \"title\":\"JSON license\",\"fun_fact\":\"The license to use JSON includes the line: The Software shall be used for Good, not Evil.\", \"url\":\"http://www.json.org/license.html\"}]}'\n\n# convert to JSON\njson_object = json.loads(json_string)\n\n# pretty print the JSON\nprint(json.dumps(json_object, indent=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.715347Z","iopub.execute_input":"2024-08-14T05:28:44.715894Z","iopub.status.idle":"2024-08-14T05:28:44.727257Z","shell.execute_reply.started":"2024-08-14T05:28:44.715862Z","shell.execute_reply":"2024-08-14T05:28:44.726053Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code below uses the tag `\"total\"` which stores the total number of returned results, and `\"newspaper\"` which stores a list of records returned by the API call. Some API endpoints, such as `/result` will limit the number of records returned, but in this case the `/newspaper/titles` endpoint returns everything. As there are almost 2 thousand newspapers recorded in Trove, the list can be very long, so the code below only returns one item from the list.","metadata":{}},{"cell_type":"code","source":"# Load the data as JSON\ndata = response.json()\n\n# Get the total number of results\ntotal = data[\"total\"]\nprint(\"Total records returned: \" + str(total))\n\n# Display the 50th result\nrecord = data[\"newspaper\"][49]\n\n## Pretty print the JSON with json.dumps\nprint(json.dumps(record, indent=4))\n\n## If you're feeling brave, uncomment this line to print all the records\n#print(json.dumps(data, indent=4))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.729071Z","iopub.execute_input":"2024-08-14T05:28:44.730059Z","iopub.status.idle":"2024-08-14T05:28:44.749258Z","shell.execute_reply.started":"2024-08-14T05:28:44.730016Z","shell.execute_reply":"2024-08-14T05:28:44.748035Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Navigating the response\n\nThe `.json()` method converts the data into a Python dictionary with keys and values. Responses may contain data, nested dictionaries, and lists. If you need a refresher on data types check out the notebook [CC Data types & structures](https://www.kaggle.com/code/sotiriosalpanis/cc-data-types-structures).\n\nTo see all the keys contained in the JSON we can use `.keys()`\n\nThe response received from Trove will include two top level keys: \n\n- `total` - that returns a count of records, and \n- `newspaper` - that returns a list of records. \n\nTo get a sense of the keys in each record, check the first newspaper record by selecting it.","metadata":{}},{"cell_type":"code","source":"# Store and print the top level keys\nkeys = data.keys()\nprint(\"The top level keys are:\")\nprint(keys)\n\n# Store and print the keys for the first newspaper in the list\nprint(\"The keys in the first newspaper record are:\")\nnewspaper_keys = data['newspaper'][0].keys()\nprint(newspaper_keys)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.750831Z","iopub.execute_input":"2024-08-14T05:28:44.751236Z","iopub.status.idle":"2024-08-14T05:28:44.762264Z","shell.execute_reply.started":"2024-08-14T05:28:44.751199Z","shell.execute_reply":"2024-08-14T05:28:44.760618Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The structure of the data means that we can select information using loops.\n\nFor example the following code finds all newspapers with the word `Age` in the title and prints the title and Id.","metadata":{}},{"cell_type":"code","source":"for record in data[\"newspaper\"]:\n    title = record.get(\"title\")\n    if \"Age\" in title:\n        print(title + \", with id: \" + record.get(\"id\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.767029Z","iopub.execute_input":"2024-08-14T05:28:44.767417Z","iopub.status.idle":"2024-08-14T05:28:44.781282Z","shell.execute_reply.started":"2024-08-14T05:28:44.767384Z","shell.execute_reply":"2024-08-14T05:28:44.779916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Headers and parameters\n\n### Headers  \n\nEach request and response will have a **header** which contains metadata about the action and common data that needs to be sent for every request, such as date of request, rate limits, and authentication.\n\nHeader parameters for Trove API requests include:\n\n- encoding: default is XML, or can be set to JSON using `\"accept\" : \"application/json\"` \n- authentication (API keys) set using `\"X-API-KEY\" : API_KEY\"`\n\nEach API will have different parameters so it is important to check the documentation.\n\nHeader parameters can be combined in a dictionary as key, value pairs.","metadata":{}},{"cell_type":"code","source":"# Headers parameter with API key and encoding.\n#API_KEY = APIkey\n#headers = {\"accept\" : \"application/json\", \"X-API-KEY\": API_KEY}\n\n# Headers with just encoding.\nencoding = {\"accept\" : \"application/json\"}","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.783044Z","iopub.execute_input":"2024-08-14T05:28:44.783451Z","iopub.status.idle":"2024-08-14T05:28:44.792065Z","shell.execute_reply.started":"2024-08-14T05:28:44.783412Z","shell.execute_reply":"2024-08-14T05:28:44.790897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Query parameters\n\nWhen requesting data from an API, other parameters called **query parameters** are often supplied to make it easier to find the data you want. These are all dependent on the specific API.\n\nThe Trove API has different parameters available when searching or when retrieveing records.\n\n**Searching**  \nThese parameters are related to the `/result` endpoint which acts like a search in the Trove catalogue, but with more granular options.\n\n- `category` : Required repeatable parameter that reflects different resource types. Options include `all`, `newspaper`, `magazine`, `image`, `research`, `book`, `diary`, `music`, `people`, `list`.\n- `q` : Optional parameter for supplying the search query.\n- `l-<facet name>` : Optional from a controlled list that includes options such as format, date, language, availability, title, etc. See the full list [Trove Technical Guide - facet values](https://trove.nla.gov.au/about/create-something/using-api/v3/api-technical-guide#facetValues-01)\n\nFurther parameters are available for navigating and sorting results and specifying what metadata should be returned. See more in the documentation: [Parameters available when searching](https://trove.nla.gov.au/about/create-something/using-api/v3/api-technical-guide#parameters-available-when-searching)\n","metadata":{}},{"cell_type":"code","source":"base_url = \"https://api.trove.nla.gov.au/v3\"\nresults_endpoint = \"/result\"\n\n# Parameters added to a dictionary that will be passed to the API call.\n# Searches for the fastest computers of the 1990s\nquery = {\"q\":\"fastest computer\", \"category\":\"all\", \"l-decade\":\"199\"}\n\n# Make GET request\nw_response = requests.get(base_url + results_endpoint, params=query, headers=encoding)\n\n# Check if response was ok.\nif w_response.status_code == requests.codes.ok:\n    print(\"The response has a status code on the OK list.\")\nelse:\n    print(\"The response status code indicates something went wrong...\")\n    \n# Show the URL used to make the request\nprint(\"API url: \")\nprint(w_response.url)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:44.793472Z","iopub.execute_input":"2024-08-14T05:28:44.795320Z","iopub.status.idle":"2024-08-14T05:28:51.622945Z","shell.execute_reply.started":"2024-08-14T05:28:44.795282Z","shell.execute_reply":"2024-08-14T05:28:51.621797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the request completed, we can now get a sense for the data. \n\nThis can involve trial and error, or looking at the documentation to find the data that interests you.\n\nThe code below prints out the number of categories, category keys and names, as well as the number of records returned for each category.\n\nThe code imports and uses methods from [IPython](https://ipython.readthedocs.io/en/stable/) to show the image thumbnail for the record returned.","metadata":{}},{"cell_type":"code","source":"# import libraries for displaying images\nfrom IPython.display import HTML, display\n\n# get the API response JSON\ndata = w_response.json()\n\nkeys = data.keys() # dict_keys(['query', 'category'])\n\ncategories = data[\"category\"] # selects the data we want to work with\n\n# print the keys\ncategory_keys = categories[0].keys()\nprint(category_keys)\n\n# loop through the list of keys and print out the details of each \n# category and how many records were returned in each\nindex = 0;\nfor category in categories:\n    print(f\"{index} Category code {category['code']}: {category['name']}, \" \n          f\"records: {category['records']['total']}\")\n    index += 1\n    \nprint(\"\")\n\n# first record in the image category note that other categories have different keywords\nfirst_image = categories[2][\"records\"][\"work\"][0]\n\n## Get the thumbnail url and display it\nimage = categories[2][\"records\"][\"work\"][0][\"identifier\"][0][\"value\"]\ndisplay(HTML('<img src=\"{}\">'.format(image)))\n\n# pretty prints the JSON record\nprint(json.dumps(first_image,indent=4))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.624602Z","iopub.execute_input":"2024-08-14T05:28:51.624977Z","iopub.status.idle":"2024-08-14T05:28:51.643487Z","shell.execute_reply.started":"2024-08-14T05:28:51.624925Z","shell.execute_reply":"2024-08-14T05:28:51.641353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## More fun with images\n\nThis is all very nice, but perhaps a bit dry if you're not a metadata librarian (like the author of this tutorial). The API can become really powerful when combined with a range of Python packages for returning different types of data. \n\nTo help demonstrate this, I copied some code from the GLAM Workbench notebook [Save Trove newspaper articles as image](https://nbviewer.org/github/GLAM-Workbench/trove-newspapers/blob/master/Save-Trove-newspaper-article-as-image.ipynb) by [Tim Sherratt](https://updates.timsherratt.org/).\n\nRunning the code below will build Sherratt's method for saving newspaper articles. It has four functions which are explained in the blocks below...","metadata":{}},{"cell_type":"code","source":"import re\nfrom io import BytesIO\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom IPython.display import HTML, display\nfrom PIL import Image\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.645231Z","iopub.execute_input":"2024-08-14T05:28:51.645612Z","iopub.status.idle":"2024-08-14T05:28:51.657855Z","shell.execute_reply.started":"2024-08-14T05:28:51.645581Z","shell.execute_reply":"2024-08-14T05:28:51.656322Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function finds the outside bounding box of the article by iterating through sets of coordinates supplied from the `get_article_boxs()` function. It returns a dictionary of the page id and edge coordinates.","metadata":{}},{"cell_type":"code","source":"def get_box(zones):\n    \"\"\"\n    Loop through all the zones to find the outer limits of each boundary.\n    Return a bounding box around the article.\n    \"\"\"\n    left = 10000\n    right = 0\n    top = 10000\n    bottom = 0\n    page_id = zones[0][\"data-page-id\"]\n    #print(zones)\n    for zone in zones:\n        if int(zone[\"data-y\"]) < top:\n            top = int(zone[\"data-y\"])\n        if int(zone[\"data-x\"]) < left:\n            left = int(zone[\"data-x\"])\n        if (int(zone[\"data-x\"]) + int(zone[\"data-w\"])) > right:\n            right = int(zone[\"data-x\"]) + int(zone[\"data-w\"])\n        if (int(zone[\"data-y\"]) + int(zone[\"data-h\"])) > bottom:\n            bottom = int(zone[\"data-y\"]) + int(zone[\"data-h\"])\n    return {\n        \"page_id\": page_id,\n        \"left\": left,\n        \"top\": top,\n        \"right\": right,\n        \"bottom\": bottom,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.659848Z","iopub.execute_input":"2024-08-14T05:28:51.661197Z","iopub.status.idle":"2024-08-14T05:28:51.676690Z","shell.execute_reply.started":"2024-08-14T05:28:51.661146Z","shell.execute_reply":"2024-08-14T05:28:51.675128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function below processes the raw web page data to find the coordinates of parts of the article. It uses Requests to retrieve the article data, which returns HTML required to view the article in a browser instead of the record structure of the Trove API. \n\nTo process the data, the function uses a library called [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) and parses the xml sections using the library [lxml](https://lxml.de/). The data contains OCR article text, which uses `<div>` tags to provide structure. Each `<div>` includes information about which page the article part is on and the boundaries of each section.\n\nThe function returns a list of boxes structured based on the output of `get_box(zones)`.","metadata":{}},{"cell_type":"code","source":"def get_article_boxes(article_url):\n    \"\"\"\n    Positional information about the article is attached to each line of the OCR output in data attributes.\n    This function loads the HTML version of the article and scrapes the x, y, and width values for each line of text\n    to determine the coordinates of a box around the article.\n    \"\"\"\n    boxes = []\n    response = requests.get(article_url)\n    soup = BeautifulSoup(response.text, \"lxml\")\n    # Lines of OCR are in divs with the class 'zone'\n    # 'onPage' limits to those on the current page\n    zones = soup.select(\"div.zone.onPage\")\n    boxes.append(get_box(zones))\n    off_page_zones = soup.select(\"div.zone.offPage\")\n    if off_page_zones:\n        current_page = off_page_zones[0][\"data-page-id\"]\n        zones = []\n        for zone in off_page_zones:\n            if zone[\"data-page-id\"] == current_page:\n                zones.append(zone)\n            else:\n                boxes.append(get_box(zones))\n                zones = [zone]\n                current_page = zone[\"data-page-id\"]\n        boxes.append(get_box(zones))\n    return boxes","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.678392Z","iopub.execute_input":"2024-08-14T05:28:51.678932Z","iopub.status.idle":"2024-08-14T05:28:51.697383Z","shell.execute_reply.started":"2024-08-14T05:28:51.678857Z","shell.execute_reply":"2024-08-14T05:28:51.695482Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next function first builds the list of boxes using the functions above. It then requests each image from the Trove image service using the page ID for each box and crops them using the coordinates. Each image is saved to memory (see `/kaggle/working` in the notebook when you run it), and returns a list of image filenames.","metadata":{}},{"cell_type":"code","source":"def get_page_images(article_id, size):\n    \"\"\"\n    Extract an image of the article from the page image(s), save it, and return the filename(s).\n    \"\"\"\n    images = []\n    # Get position of article on the page(s)\n    boxes = get_article_boxes(\"http://nla.gov.au/nla.news-article{}\".format(article_id))\n    for box in boxes:\n        # print(box)\n        # Construct the url we need to download the page image\n        page_url = (\n            \"https://trove.nla.gov.au/ndp/imageservice/nla.news-page{}/level{}\".format(\n                box[\"page_id\"], 7\n            )\n        )\n        # Download the page image\n        response = requests.get(page_url)\n        # Open download as an image for editing\n        img = Image.open(BytesIO(response.content))\n        # Use coordinates of top line to create a square box to crop thumbnail\n        points = (box[\"left\"], box[\"top\"], box[\"right\"], box[\"bottom\"])\n        # Crop image to article box\n        cropped = img.crop(points)\n        # Resize if necessary\n        if size:\n            cropped.thumbnail((size, size), Image.LANCZOS)\n        # Save and display thumbnail\n        cropped_file = \"nla.news-article{}-{}.jpg\".format(article_id, box[\"page_id\"])\n        cropped.save(cropped_file)\n        images.append(cropped_file)\n    return images","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.699420Z","iopub.execute_input":"2024-08-14T05:28:51.699850Z","iopub.status.idle":"2024-08-14T05:28:51.712940Z","shell.execute_reply.started":"2024-08-14T05:28:51.699813Z","shell.execute_reply":"2024-08-14T05:28:51.711090Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The final function below brings all these functions together to process an article into a cohesive image. \n\nFirst, it searches the `article_url` (equivalent to the `troveUrl` element in the API records) for the article id and uses it to get images. \n\nThe size parameter limits the maximum size of the images. Use `None` to get full size. Once the image files have been downloaded by the `get_page_images()` function, it uses `HTML()` and `display()` from [IPython.display](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) library to show the images in the output.","metadata":{}},{"cell_type":"code","source":"def get_article(article_url, size):\n    # Get the article record from the API\n    article_id = re.search(r\"article\\/{0,1}(\\d+)\", article_url).group(1)\n    # print(article_id)\n    images = get_page_images(article_id, size)\n    for image in images:\n        display(HTML(f'<a href=\"{image}\" download>Download {image}</a>'))\n        display(HTML('<img src=\"{}\">'.format(image)))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.715298Z","iopub.execute_input":"2024-08-14T05:28:51.715880Z","iopub.status.idle":"2024-08-14T05:28:51.731354Z","shell.execute_reply.started":"2024-08-14T05:28:51.715796Z","shell.execute_reply":"2024-08-14T05:28:51.729150Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that the functions have been added, we can use `get_article` to display some images from the API response. \n\nThe below code will create a list of all the newspaper articles, find all the Trove URLs in the data, create a list, and run `get_article` on the first 3. \n\nYou can try updating this by:\n\n- Adjusting the existing inputs like size or the start and end of the range,\n- Creating a new API call on a topic that interests you,\n- Creating functions to work with images from other data categories. *This will require creating new functions using the structure of the URL and the different JSON elements for each category.*","metadata":{}},{"cell_type":"code","source":"# Get the results from the newspaper category\narticles_list = categories[6][\"records\"][\"article\"]\nto_get = []\n\n# get all the TroveUrl values from the data\nfor article in articles_list:\n    if 'troveUrl' in article.keys():\n        to_get.append(article['troveUrl'])\n\n# sort list\nto_get.sort()\n\n# calls the first three images in the list\nfor i in range(1):\n    # prints the Trove URL\n    print(to_get[i])\n    # calls the get_article function on the URL\n    get_article(to_get[i], 400)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T05:28:51.733267Z","iopub.execute_input":"2024-08-14T05:28:51.733845Z","iopub.status.idle":"2024-08-14T05:28:56.963993Z","shell.execute_reply.started":"2024-08-14T05:28:51.733778Z","shell.execute_reply":"2024-08-14T05:28:56.962764Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next steps\n\nIf you have made it this far, you now have the knowledge to be able to create a similar function to the one used at the start of this notebook. You have seen how to construct a valid GET request, including configuring header and query parameters, and how to navigate the data returned by the Trove API. \nTweak the code to get familiar with how to use the API or explore one of the tutorials below.\n\n- Check out the documentation for version 3 [Using APIs](https://trove.nla.gov.au/about/create-something/using-api)\n- Explore tutorials from the [Trove Data guide](https://tdg.glam-workbench.net/home.html) such as:\n    - Learn about retrieving multiple pages of results: [Harvest a complete set of search results using the Trove API](https://tdg.glam-workbench.net/accessing-data/how-to/harvest-complete-results.html)\n    - Learn more about [Accessing data about newspaper and gazette articles](https://tdg.glam-workbench.net/newspapers-and-gazettes/data/articles.html)\n    - Investigate other digitised resources [Understanding and using digitised resources](https://tdg.glam-workbench.net/other-digitised-resources/index.html) such as books and oral histories \n- Learn more about APIs through [API for Libraries at Library Carpentry](https://joshuadull.github.io/APIs-for-Libraries/)\n","metadata":{}},{"cell_type":"markdown","source":"## Projects built using Trove API\n\n### Ideas and tools\n\n[GLAM workbench](https://glam-workbench.net/) has a lot of Jupyter notebooks covering Trove API projects including [making a IIIF manifest for display in IIIF viewers](https://glam-workbench.net/trove-images/save-image-collection-iiif/#using-this-notebook) or [get OCR text from digitised journals](https://glam-workbench.net/trove-journals/get-ocrd-text-from-digitised-journal/). Also check out [Tim Sherrat's blog](https://updates.timsherratt.org/) for updates on his work developing for the GLAM workbench.\n\n### Projects that use the Trove API\n\n[digitalpasifik.org/](https://digitalpasifik.org/) - a pilot project as part of the Pacific Virtual Museum project aiming to bring together digitised Pacific collections held at many institutions into one location to improve access. \n\n[Drifter](https://mtchl.net/drifter/) - 2016 work by Mitchell Whitelaw that brings together data relating to the Murrumbidgee river system, including newspapers from the Trove API with other tools and datasets.\n\n[To Be Continued...](https://bootstrap.rbi.skyhigh.cloud/clientless/#url=https://readallaboutit.com.au/) Australian Newspaper Fiction database creating a searchable collection of 40,000 stories from the 19th and 20th centuries.\n\n","metadata":{}}]}